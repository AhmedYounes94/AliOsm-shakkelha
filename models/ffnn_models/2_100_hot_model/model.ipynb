{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '../../../dataset'\n",
    "CHARS_NUM = 50\n",
    "CLASSES_NUM = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = None\n",
    "Y_train = None\n",
    "with open(DATASET_PATH + '/X_train.pickle', 'rb') as X_train_file, \\\n",
    "         open(DATASET_PATH + '/Y_train.pickle', 'rb') as Y_train_file:\n",
    "    X_train = pkl.load(X_train_file)\n",
    "    Y_train = pkl.load(Y_train_file)\n",
    "print('Training examples:', len(X_train))\n",
    "\n",
    "X_val = None\n",
    "Y_val = None\n",
    "with open(DATASET_PATH + '/X_val.pickle', 'rb') as X_val_file, \\\n",
    "         open(DATASET_PATH + '/Y_val.pickle', 'rb') as Y_val_file:\n",
    "    X_val = pkl.load(X_val_file)\n",
    "    Y_val = pkl.load(Y_val_file)\n",
    "print('Validation examples:', len(X_val))\n",
    "\n",
    "X_test = None\n",
    "Y_test = None\n",
    "with open(DATASET_PATH + '/X_test.pickle', 'rb') as X_test_file, \\\n",
    "         open(DATASET_PATH + '/Y_test.pickle', 'rb') as Y_test_file:\n",
    "    X_test = pkl.load(X_test_file)\n",
    "    Y_test = pkl.load(Y_test_file)\n",
    "print('Testing examples:', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def one_hot(input_dim=None, input_length=None):\n",
    "    def _one_hot(x, classes_num):\n",
    "        return keras.backend.one_hot(keras.backend.cast(x, 'uint8'),\n",
    "                                     num_classes=classes_num)\n",
    "\n",
    "    return keras.layers.Lambda(_one_hot,\n",
    "                               arguments={'classes_num': input_dim},\n",
    "                               input_shape=(input_length,))\n",
    "\n",
    "def create_dense_layer(neurons):\n",
    "    return keras.layers.Dense(neurons,\n",
    "                              activation=tf.nn.relu,\n",
    "                              kernel_initializer=keras.initializers.glorot_normal(seed=961))\n",
    "\n",
    "def create_model(dropout_factor):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        one_hot(83, 2 * CHARS_NUM),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dropout(dropout_factor),\n",
    "        create_dense_layer(250),\n",
    "        keras.layers.Dropout(dropout_factor),\n",
    "        create_dense_layer(200),\n",
    "        keras.layers.Dropout(dropout_factor),\n",
    "        create_dense_layer(150),\n",
    "        keras.layers.Dropout(dropout_factor),\n",
    "        create_dense_layer(100),\n",
    "        keras.layers.Dropout(dropout_factor),\n",
    "        create_dense_layer(50),\n",
    "        keras.layers.Dropout(dropout_factor),\n",
    "        keras.layers.Dense(CLASSES_NUM,\n",
    "                           activation=tf.nn.softmax,\n",
    "                           kernel_initializer=keras.initializers.glorot_normal(seed=961))\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                  optimizer=tf.train.AdamOptimizer(),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_model(0.025)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, X, Y, batch_size):\n",
    "        self.X, self.Y = X, Y\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_batch = np.asarray(self.X[idx * self.batch_size:(idx + 1) * self.batch_size])\n",
    "        Y_batch = np.asarray(self.Y[idx * self.batch_size:(idx + 1) * self.batch_size])\n",
    "        \n",
    "        X_tmp = list()\n",
    "        for x in X_batch:\n",
    "            before_need = x[0]\n",
    "            after_need = x[-1]\n",
    "            x_new = list()\n",
    "            x_new.extend([1] * before_need)\n",
    "            x_new.extend(x[1:-1])\n",
    "            x_new.extend([1] * after_need)\n",
    "            X_tmp.append(np.asarray(x_new))\n",
    "        \n",
    "        X_batch = np.asarray(X_tmp)\n",
    "        Y_batch = np.asarray(Y_batch)\n",
    "        \n",
    "        return X_batch, Y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def fit_model(model, epochs, batch_size):\n",
    "    checkpoint_path = 'checkpoints/epoch{epoch:02d}.ckpt'\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, verbose=0)\n",
    "    \n",
    "    training_generator = DataGenerator(X_train, Y_train, batch_size)\n",
    "    val_generator = DataGenerator(X_val, Y_val, batch_size)\n",
    "\n",
    "    model.fit_generator(generator=training_generator,\n",
    "                        validation_data=val_generator,\n",
    "                        epochs=epochs,\n",
    "                        callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "fit_model(model, 50, 512)\n",
    "end_time = time.time()\n",
    "print(\"--- %s seconds ---\" % round(end_time - start_time, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = DataGenerator(X_test, Y_test, len(X_test))\n",
    "\n",
    "XY_test = list()\n",
    "for batch in range(len(test_generator)):\n",
    "    XY_test.append(test_generator[batch])\n",
    "X, Y = zip(*XY_test)\n",
    "\n",
    "X = np.asarray(X)\n",
    "Y = np.asarray(Y)\n",
    "X = np.squeeze(X)\n",
    "Y = np.squeeze(Y)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X, Y, batch_size=512)\n",
    "print('Accuracy: %s%%' % round(acc * 100, 2))\n",
    "print('Loss: %s' % round(loss, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
